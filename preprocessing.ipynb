{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pdfplumber in c:\\users\\ashwin v kumar\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (0.11.5)\n",
      "Requirement already satisfied: pandas in c:\\users\\ashwin v kumar\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: nltk in c:\\users\\ashwin v kumar\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\ashwin v kumar\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (0.9.0)\n",
      "Requirement already satisfied: pdfminer.six==20231228 in c:\\users\\ashwin v kumar\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pdfplumber) (20231228)\n",
      "Requirement already satisfied: Pillow>=9.1 in c:\\users\\ashwin v kumar\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pdfplumber) (11.1.0)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in c:\\users\\ashwin v kumar\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pdfplumber) (4.30.1)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\users\\ashwin v kumar\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pdfminer.six==20231228->pdfplumber) (3.4.1)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\ashwin v kumar\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pdfminer.six==20231228->pdfplumber) (44.0.1)\n",
      "Requirement already satisfied: numpy>=1.22.4 in c:\\users\\ashwin v kumar\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas) (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\ashwin v kumar\\appdata\\roaming\\python\\python39\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ashwin v kumar\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\ashwin v kumar\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: click in c:\\users\\ashwin v kumar\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\ashwin v kumar\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\ashwin v kumar\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ashwin v kumar\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\ashwin v kumar\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ashwin v kumar\\appdata\\roaming\\python\\python39\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ashwin v kumar\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ashwin v kumar\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ashwin v kumar\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\n",
      "Requirement already satisfied: colorama in c:\\users\\ashwin v kumar\\appdata\\roaming\\python\\python39\\site-packages (from click->nltk) (0.4.6)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\ashwin v kumar\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.17.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\ashwin v kumar\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pdfplumber pandas nltk tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Processing: ConsumerProtectionAct2019.pdf ...\n",
      "‚úÖ Saved extracted text to C:\\Users\\Ashwin V Kumar\\OneDrive\\Desktop\\DataPP\\ConsumerProtectionAct2019.txt\n",
      "üìÑ Processing: Insurance-Act-1938.pdf ...\n",
      "‚úÖ Saved extracted text to C:\\Users\\Ashwin V Kumar\\OneDrive\\Desktop\\DataPP\\Insurance-Act-1938.txt\n",
      "üìÑ Processing: MotorVehicleAct1988.pdf ...\n",
      "‚úÖ Saved extracted text to C:\\Users\\Ashwin V Kumar\\OneDrive\\Desktop\\DataPP\\MotorVehicleAct1988.txt\n",
      "üìÑ Processing: the_code_of_criminal_procedure,_1973.pdf ...\n",
      "‚úÖ Saved extracted text to C:\\Users\\Ashwin V Kumar\\OneDrive\\Desktop\\DataPP\\the_code_of_criminal_procedure,_1973.txt\n",
      "üìÑ Processing: the_indian_penal_code,_1860.pdf ...\n",
      "‚úÖ Saved extracted text to C:\\Users\\Ashwin V Kumar\\OneDrive\\Desktop\\DataPP\\the_indian_penal_code,_1860.txt\n",
      "\n",
      "üöÄ All PDFs processed successfully!\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import os\n",
    "\n",
    "# Define the folder path\n",
    "pdf_folder = \"C:\\\\Users\\\\Ashwin V Kumar\\\\OneDrive\\\\Desktop\\\\DataPP\"\n",
    "\n",
    "# Ensure the folder exists\n",
    "if not os.path.exists(pdf_folder):\n",
    "    print(f\"‚ùå Folder '{pdf_folder}' not found!\")\n",
    "    exit()\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extracts text from a given PDF file.\"\"\"\n",
    "    text = \"\"\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:\n",
    "                text += page_text + \"\\n\"\n",
    "    return text.strip()\n",
    "\n",
    "# Get all PDFs in the \"datapp\" directory\n",
    "pdf_files = [f for f in os.listdir(pdf_folder) if f.endswith(\".pdf\")]\n",
    "\n",
    "# Dictionary to store extracted text\n",
    "legal_texts = {}\n",
    "\n",
    "# Process all PDFs\n",
    "for pdf in pdf_files:\n",
    "    pdf_path = os.path.join(pdf_folder, pdf)\n",
    "    print(f\"üìÑ Processing: {pdf} ...\")\n",
    "    \n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    legal_texts[pdf] = text\n",
    "    \n",
    "    # Save extracted text to a file\n",
    "    txt_filename = os.path.join(pdf_folder, pdf.replace(\".pdf\", \".txt\"))\n",
    "    with open(txt_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text)\n",
    "\n",
    "    print(f\"‚úÖ Saved extracted text to {txt_filename}\")\n",
    "\n",
    "print(\"\\nüöÄ All PDFs processed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All extracted text files have been loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the path where text files are stored\n",
    "txt_folder = \"C:/Users/Ashwin V Kumar/OneDrive/Desktop/DataPP\"\n",
    "\n",
    "# Get all text files in the folder\n",
    "txt_files = [f for f in os.listdir(txt_folder) if f.endswith(\".txt\")]\n",
    "\n",
    "# Dictionary to store the loaded text\n",
    "legal_documents = {}\n",
    "\n",
    "# Load each text file\n",
    "for txt_file in txt_files:\n",
    "    txt_path = os.path.join(txt_folder, txt_file)\n",
    "    with open(txt_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        legal_documents[txt_file] = file.read()\n",
    "\n",
    "print(\"‚úÖ All extracted text files have been loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Text cleaning completed!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Cleans extracted text by removing unwanted elements like extra spaces, page numbers, etc.\"\"\"\n",
    "    text = re.sub(r\"\\n+\", \"\\n\", text)  # Remove extra newlines\n",
    "    text = re.sub(r\"Page \\d+\", \"\", text)  # Remove page numbers (e.g., \"Page 12\")\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # Replace multiple spaces with a single space\n",
    "    text = text.strip()  # Remove leading/trailing spaces\n",
    "    return text\n",
    "\n",
    "# Apply cleaning to all documents\n",
    "for filename in legal_documents:\n",
    "    legal_documents[filename] = clean_text(legal_documents[filename])\n",
    "\n",
    "print(\"‚úÖ Text cleaning completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\ashwin v kumar\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\ashwin v kumar\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\ashwin v kumar\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\ashwin v kumar\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ashwin v kumar\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\ashwin v kumar\\appdata\\roaming\\python\\python39\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\Ashwin V\n",
      "[nltk_data]     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tokenized and saved: ConsumerProtectionAct2019.txt\n",
      "‚úÖ Tokenized and saved: Insurance-Act-1938.txt\n",
      "‚úÖ Tokenized and saved: MotorVehicleAct1988.txt\n",
      "‚úÖ Tokenized and saved: the_code_of_criminal_procedure,_1973.txt\n",
      "‚úÖ Tokenized and saved: the_indian_penal_code,_1860.txt\n",
      "\n",
      "‚úÖ Sentence tokenization completed! Tokenized files are saved in: C:\\Users\\Ashwin V Kumar\\OneDrive\\Desktop\\DataPP\\output\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "BASE_DIR = r\"C:\\Users\\Ashwin V Kumar\\OneDrive\\Desktop\\DataPP\"\n",
    "OUTPUT_DIR = os.path.join(BASE_DIR, \"output\")\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "# Step 3: Process each text file in the directory\n",
    "for filename in os.listdir(BASE_DIR):\n",
    "    file_path = os.path.join(BASE_DIR, filename)\n",
    "\n",
    "    # Ensure it's a text file and not the output folder\n",
    "    if filename.endswith(\".txt\") and filename != \"output\":\n",
    "        try:\n",
    "            # Read file content\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                text = file.read().strip()\n",
    "\n",
    "            # Skip empty files\n",
    "            if not text:\n",
    "                print(f\"‚ö†Ô∏è Skipped empty file: {filename}\")\n",
    "                continue\n",
    "\n",
    "            # Tokenize into sentences\n",
    "            sentences = sent_tokenize(text)\n",
    "\n",
    "            # Save tokenized sentences in output folder\n",
    "            output_file_path = os.path.join(OUTPUT_DIR, f\"tokenized_{filename}\")\n",
    "            with open(output_file_path, \"w\", encoding=\"utf-8\") as output_file:\n",
    "                output_file.write(\"\\n\".join(sentences))\n",
    "\n",
    "            print(f\"‚úÖ Tokenized and saved: {filename}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing {filename}: {e}\")\n",
    "\n",
    "print(\"\\n‚úÖ Sentence tokenization completed! Tokenized files are saved in:\", OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Ashwin V\n",
      "[nltk_data]     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Ashwin V\n",
      "[nltk_data]     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Ashwin V\n",
      "[nltk_data]     Kumar\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Processed: ConsumerProtectionAct2019.txt\n",
      "‚úÖ Processed: Insurance-Act-1938.txt\n",
      "‚úÖ Processed: MotorVehicleAct1988.txt\n",
      "‚úÖ Processed: the_code_of_criminal_procedure,_1973.txt\n",
      "‚úÖ Processed: the_indian_penal_code,_1860.txt\n",
      "\n",
      "üéØ Text processing completed! Processed files are saved in: C:\\Users\\Ashwin V Kumar\\OneDrive\\Desktop\\DataPP\\output\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# ‚úÖ Ensure required NLTK data is downloaded\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "# ‚úÖ Define input & output directories\n",
    "BASE_DIR = r\"C:\\Users\\Ashwin V Kumar\\OneDrive\\Desktop\\DataPP\"  # Update this to your actual path\n",
    "OUTPUT_DIR = os.path.join(BASE_DIR, \"output\")\n",
    "\n",
    "# ‚úÖ Create output directory if it doesn't exist\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# ‚úÖ Initialize stopwords and lemmatizer\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# ‚úÖ Function to preprocess text\n",
    "def preprocess_text(sentence):\n",
    "    words = word_tokenize(sentence)  # Tokenize into words\n",
    "    words = [word.lower() for word in words if word.isalnum()]  # Remove special characters\n",
    "    words = [word for word in words if word not in stop_words]  # Remove stopwords\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]  # Lemmatization\n",
    "    return \" \".join(words)\n",
    "\n",
    "# ‚úÖ Dictionary to store preprocessed sentences\n",
    "processed_legal_documents = {}\n",
    "\n",
    "# ‚úÖ Read, tokenize, preprocess, and save results\n",
    "for filename in os.listdir(BASE_DIR):\n",
    "    file_path = os.path.join(BASE_DIR, filename)\n",
    "\n",
    "    if filename.endswith(\".txt\") and filename != \"output\":  # Process only .txt files\n",
    "        try:\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                text = file.read().strip()\n",
    "\n",
    "            if text:  # Skip empty files\n",
    "                sentences = sent_tokenize(text)  # Sentence tokenization\n",
    "                preprocessed_sentences = [preprocess_text(sent) for sent in sentences]  # Apply preprocessing\n",
    "                processed_legal_documents[filename] = preprocessed_sentences\n",
    "\n",
    "                print(f\"‚úÖ Processed: {filename}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing {filename}: {e}\")\n",
    "\n",
    "# ‚úÖ Save the preprocessed sentences into the output directory\n",
    "for filename, sentences in processed_legal_documents.items():\n",
    "    output_file_path = os.path.join(OUTPUT_DIR, f\"processed_{filename}\")\n",
    "    \n",
    "    with open(output_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(sentences))  # Save each sentence on a new line\n",
    "\n",
    "print(\"\\nüéØ Text processing completed! Processed files are saved in:\", OUTPUT_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
